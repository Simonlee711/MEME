{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de83a1f9-fc4e-45aa-be1f-5c603478d5c2",
   "metadata": {},
   "source": [
    "# Training Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8bccec5-90ae-4be4-ae6e-287348d566b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./ImageBind\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d\n",
      "  Using cached pytorchvideo-0.1.5-py3-none-any.whl\n",
      "Requirement already satisfied: torch==1.13.0 in /opt/conda/lib/python3.9/site-packages (from imagebind==0.1.0) (1.13.0)\n",
      "Requirement already satisfied: torchvision==0.14.0 in /opt/conda/lib/python3.9/site-packages (from imagebind==0.1.0) (0.14.0)\n",
      "Requirement already satisfied: torchaudio==0.13.0 in /opt/conda/lib/python3.9/site-packages (from imagebind==0.1.0) (0.13.0)\n",
      "Requirement already satisfied: timm==0.6.7 in /opt/conda/lib/python3.9/site-packages (from imagebind==0.1.0) (0.6.7)\n",
      "Requirement already satisfied: ftfy in /opt/conda/lib/python3.9/site-packages (from imagebind==0.1.0) (6.1.1)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.9/site-packages (from imagebind==0.1.0) (2023.8.8)\n",
      "Requirement already satisfied: einops in /opt/conda/lib/python3.9/site-packages (from imagebind==0.1.0) (0.6.0)\n",
      "Requirement already satisfied: fvcore in /opt/conda/lib/python3.9/site-packages (from imagebind==0.1.0) (0.1.5.post20221221)\n",
      "Requirement already satisfied: eva-decord==0.6.1 in /opt/conda/lib/python3.9/site-packages (from imagebind==0.1.0) (0.6.1)\n",
      "Requirement already satisfied: iopath in /opt/conda/lib/python3.9/site-packages (from imagebind==0.1.0) (0.1.10)\n",
      "Requirement already satisfied: numpy>=1.19 in /opt/conda/lib/python3.9/site-packages (from imagebind==0.1.0) (1.23.5)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (from imagebind==0.1.0) (3.6.3)\n",
      "Requirement already satisfied: types-regex in /opt/conda/lib/python3.9/site-packages (from imagebind==0.1.0) (2023.8.8.0)\n",
      "Requirement already satisfied: mayavi in /opt/conda/lib/python3.9/site-packages (from imagebind==0.1.0) (4.8.1)\n",
      "Requirement already satisfied: cartopy in /opt/conda/lib/python3.9/site-packages (from imagebind==0.1.0) (0.22.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch==1.13.0->imagebind==0.1.0) (4.7.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.9/site-packages (from torch==1.13.0->imagebind==0.1.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.9/site-packages (from torch==1.13.0->imagebind==0.1.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.9/site-packages (from torch==1.13.0->imagebind==0.1.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.9/site-packages (from torch==1.13.0->imagebind==0.1.0) (11.10.3.66)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.9/site-packages (from torchvision==0.14.0->imagebind==0.1.0) (9.4.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from torchvision==0.14.0->imagebind==0.1.0) (2.28.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->imagebind==0.1.0) (65.6.3)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->imagebind==0.1.0) (0.38.4)\n",
      "Requirement already satisfied: pyshp>=2.1 in /opt/conda/lib/python3.9/site-packages (from cartopy->imagebind==0.1.0) (2.3.1)\n",
      "Requirement already satisfied: shapely>=1.7 in /opt/conda/lib/python3.9/site-packages (from cartopy->imagebind==0.1.0) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20 in /opt/conda/lib/python3.9/site-packages (from cartopy->imagebind==0.1.0) (23.0)\n",
      "Requirement already satisfied: pyproj>=3.1.0 in /opt/conda/lib/python3.9/site-packages (from cartopy->imagebind==0.1.0) (3.6.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib->imagebind==0.1.0) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib->imagebind==0.1.0) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->imagebind==0.1.0) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib->imagebind==0.1.0) (4.38.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->imagebind==0.1.0) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->imagebind==0.1.0) (1.0.7)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.9/site-packages (from ftfy->imagebind==0.1.0) (0.2.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from fvcore->imagebind==0.1.0) (6.0.1)\n",
      "Requirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.9/site-packages (from fvcore->imagebind==0.1.0) (2.3.0)\n",
      "Requirement already satisfied: yacs>=0.1.6 in /opt/conda/lib/python3.9/site-packages (from fvcore->imagebind==0.1.0) (0.1.8)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from fvcore->imagebind==0.1.0) (4.64.1)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.9/site-packages (from fvcore->imagebind==0.1.0) (0.9.0)\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.9/site-packages (from iopath->imagebind==0.1.0) (2.7.0)\n",
      "Requirement already satisfied: vtk in /opt/conda/lib/python3.9/site-packages (from mayavi->imagebind==0.1.0) (9.2.6)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.9/site-packages (from mayavi->imagebind==0.1.0) (2.14.0)\n",
      "Requirement already satisfied: traits>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from mayavi->imagebind==0.1.0) (6.4.2)\n",
      "Requirement already satisfied: traitsui>=7.0.0 in /opt/conda/lib/python3.9/site-packages (from mayavi->imagebind==0.1.0) (8.0.0)\n",
      "Requirement already satisfied: envisage in /opt/conda/lib/python3.9/site-packages (from mayavi->imagebind==0.1.0) (7.0.3)\n",
      "Requirement already satisfied: apptools in /opt/conda/lib/python3.9/site-packages (from mayavi->imagebind==0.1.0) (5.2.1)\n",
      "Requirement already satisfied: pyface>=6.1.1 in /opt/conda/lib/python3.9/site-packages (from mayavi->imagebind==0.1.0) (8.0.0)\n",
      "Requirement already satisfied: parameterized in /opt/conda/lib/python3.9/site-packages (from pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d->imagebind==0.1.0) (0.9.0)\n",
      "Requirement already satisfied: av in /opt/conda/lib/python3.9/site-packages (from pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d->imagebind==0.1.0) (10.0.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.9/site-packages (from pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d->imagebind==0.1.0) (3.0)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /opt/conda/lib/python3.9/site-packages (from pyface>=6.1.1->mayavi->imagebind==0.1.0) (4.13.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.9/site-packages (from pyproj>=3.1.0->cartopy->imagebind==0.1.0) (2022.12.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->imagebind==0.1.0) (1.16.0)\n",
      "Requirement already satisfied: configobj in /opt/conda/lib/python3.9/site-packages (from apptools->mayavi->imagebind==0.1.0) (5.0.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->torchvision==0.14.0->imagebind==0.1.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->torchvision==0.14.0->imagebind==0.1.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->torchvision==0.14.0->imagebind==0.1.0) (1.26.14)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=3.6->pyface>=6.1.1->mayavi->imagebind==0.1.0) (3.13.0)\n",
      "Building wheels for collected packages: imagebind\n",
      "  Building wheel for imagebind (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for imagebind: filename=imagebind-0.1.0-py3-none-any.whl size=27969 sha256=ce8877ee6f6638063217f6eb5dc7e46f778b0f855909676d32eba4c46fb1a607\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-zik8ykv5/wheels/15/d5/57/9f60b1256b436b67dceac96201d8bc2eebd9fd320f633e190f\n",
      "Successfully built imagebind\n",
      "Installing collected packages: imagebind\n",
      "  Attempting uninstall: imagebind\n",
      "    Found existing installation: imagebind 0.1.0\n",
      "    Uninstalling imagebind-0.1.0:\n",
      "      Successfully uninstalled imagebind-0.1.0\n",
      "Successfully installed imagebind-0.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-gh6dt6e5\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-gh6dt6e5\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 172f42c512e1bf32554ef910fe82f07916b4d4af\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (1.23.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (2.28.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (3.12.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (4.64.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (0.3.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (0.13.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (2023.8.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.34.0.dev0) (4.7.1)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.34.0.dev0) (2023.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.34.0.dev0) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.34.0.dev0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.34.0.dev0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.34.0.dev0) (2.1.1)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.34.0.dev0-py3-none-any.whl size=7638341 sha256=f20b217ef195cbce1a9e76b5773898232c34e3649ec2b9c2088781dc26860364\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-c7q7jchd/wheels/f7/92/8c/752ff3bfcd3439805d8bbf641614da38ef3226e127ebea86ee\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.26.1\n",
      "    Uninstalling transformers-4.26.1:\n",
      "      Successfully uninstalled transformers-4.26.1\n",
      "Successfully installed transformers-4.34.0.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: sagemaker>=2.140.0 in /opt/conda/lib/python3.9/site-packages (2.183.0)\n",
      "Collecting transformers==4.26.1\n",
      "  Using cached transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "Requirement already satisfied: datasets[s3]==2.10.1 in /opt/conda/lib/python3.9/site-packages (2.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (1.23.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (6.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (3.12.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (2.28.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (0.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (0.16.4)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (0.3.6)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (2023.1.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (3.3.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (11.0.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (1.5.3)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (0.18.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (3.8.5)\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (0.4.2)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (3.20.2)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (23.1.0)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (4.13.0)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (0.7.5)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (0.3.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (3.10.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (1.0.1)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (2.2.1)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (1.28.41)\n",
      "Requirement already satisfied: tblib==1.7.0 in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (1.7.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (4.19.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from boto3<2.0,>=1.26.131->sagemaker>=2.140.0) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from boto3<2.0,>=1.26.131->sagemaker>=2.140.0) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.41 in /opt/conda/lib/python3.9/site-packages (from boto3<2.0,>=1.26.131->sagemaker>=2.140.0) (1.31.41)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets[s3]==2.10.1) (2.1.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets[s3]==2.10.1) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets[s3]==2.10.1) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets[s3]==2.10.1) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets[s3]==2.10.1) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets[s3]==2.10.1) (6.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.1) (4.7.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker>=2.140.0) (3.13.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.26.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.26.1) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.26.1) (2022.12.7)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from google-pasta->sagemaker>=2.140.0) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.9/site-packages (from jsonschema->sagemaker>=2.140.0) (2023.7.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from jsonschema->sagemaker>=2.140.0) (0.10.2)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.9/site-packages (from jsonschema->sagemaker>=2.140.0) (0.30.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets[s3]==2.10.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets[s3]==2.10.1) (2022.7.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.9/site-packages (from pathos->sagemaker>=2.140.0) (1.7.6.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.9/site-packages (from pathos->sagemaker>=2.140.0) (0.3.2)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.9/site-packages (from schema->sagemaker>=2.140.0) (21.6.0)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.34.0.dev0\n",
      "    Uninstalling transformers-4.34.0.dev0:\n",
      "      Successfully uninstalled transformers-4.34.0.dev0\n",
      "Successfully installed transformers-4.26.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: accelerate==0.20.3 in /opt/conda/lib/python3.9/site-packages (0.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from accelerate==0.20.3) (23.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from accelerate==0.20.3) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from accelerate==0.20.3) (1.23.5)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate==0.20.3) (5.9.4)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from accelerate==0.20.3) (1.13.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.9/site-packages (from torch>=1.6.0->accelerate==0.20.3) (8.5.0.96)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.6.0->accelerate==0.20.3) (4.7.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.9/site-packages (from torch>=1.6.0->accelerate==0.20.3) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.9/site-packages (from torch>=1.6.0->accelerate==0.20.3) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.9/site-packages (from torch>=1.6.0->accelerate==0.20.3) (11.10.3.66)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->accelerate==0.20.3) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->accelerate==0.20.3) (65.6.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ImageBind/.\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install \"sagemaker>=2.140.0\" \"transformers==4.26.1\" \"datasets[s3]==2.10.1\" --upgrade\n",
    "!pip install accelerate==0.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01051945-9ada-4fc2-bb87-339ccd3480a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.9/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from imagebind import data\n",
    "import torch\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType\n",
    "\n",
    "import numpy as np\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO # Python 3.\n",
    "from datasets import load_dataset,Dataset,DatasetDict,concatenate_datasets\n",
    "\n",
    "from transformers import DataCollatorWithPadding,AutoModelForSequenceClassification, Trainer, TrainingArguments,AutoTokenizer,AutoModel,AutoConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f342b3f-190d-4a93-9d95-15513b2b7358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mimic-iv-2.2/', 'mimic-iv-clinical-database-demo-2.2/', 'mimic-iv-ed-2.2/', 'mimic-iv-ed-demo-2.2/']\n"
     ]
    }
   ],
   "source": [
    "# print bucket contents to see which one we are working with\n",
    "s3 = boto3.client('s3')\n",
    "bucket = 'chianglab-dataderivatives'\n",
    "subfolder = 'mimic-iv-clinical-database-2.2'\n",
    "\n",
    "response = s3.list_objects_v2(Bucket=bucket)\n",
    "folders = []\n",
    "for obj in response['Contents']:\n",
    "    if obj['Key'].endswith('/'):\n",
    "        folders.append(obj['Key'])\n",
    "print(folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6596f270-8527-41ae-8b24-1b4b3d71e6c6",
   "metadata": {},
   "source": [
    "# Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b40f3fc6-2fe7-4a8f-8838-fef89edeb1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataframe: 400019\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arrival</th>\n",
       "      <th>eddischarge</th>\n",
       "      <th>admission</th>\n",
       "      <th>discharge</th>\n",
       "      <th>eddischarge_category</th>\n",
       "      <th>triage</th>\n",
       "      <th>medrecon</th>\n",
       "      <th>vitals</th>\n",
       "      <th>codes</th>\n",
       "      <th>pyxis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33258284</th>\n",
       "      <td>Patient 10000032, a 52 year old white female, ...</td>\n",
       "      <td>The ED disposition was admitted at 2180-05-06 ...</td>\n",
       "      <td>The patient was admitted at 2180-05-06 22:23:00.</td>\n",
       "      <td>The patient's discharge disposition was: home ...</td>\n",
       "      <td>a d m i t t e d</td>\n",
       "      <td>At triage: temperature was 98.4, pulse was 70....</td>\n",
       "      <td>The patient was previously taking the followin...</td>\n",
       "      <td>The patient had the following vitals: At 2180-...</td>\n",
       "      <td>The patient received the following diagnostic ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38112554</th>\n",
       "      <td>Patient 10000032, a 52 year old white female, ...</td>\n",
       "      <td>The ED disposition was admitted at 2180-06-26 ...</td>\n",
       "      <td>The patient was admitted at 2180-06-26 18:27:00.</td>\n",
       "      <td>The patient's discharge disposition was: home ...</td>\n",
       "      <td>a d m i t t e d</td>\n",
       "      <td>At triage: temperature was 98.9, pulse was 88....</td>\n",
       "      <td>The patient was previously taking the followin...</td>\n",
       "      <td>The patient had the following vitals: At 2180-...</td>\n",
       "      <td>The patient received the following diagnostic ...</td>\n",
       "      <td>The patient received the following medications...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35968195</th>\n",
       "      <td>Patient 10000032, a 52 year old white female, ...</td>\n",
       "      <td>The ED disposition was admitted at 2180-08-06 ...</td>\n",
       "      <td>The patient was admitted at 2180-08-05 23:44:00.</td>\n",
       "      <td>The patient's discharge disposition was: hospi...</td>\n",
       "      <td>a d m i t t e d</td>\n",
       "      <td>At triage: temperature was 99.4, pulse was 105...</td>\n",
       "      <td>The patient was previously taking the followin...</td>\n",
       "      <td>The patient had the following vitals: At 2180-...</td>\n",
       "      <td>The patient received the following diagnostic ...</td>\n",
       "      <td>The patient received the following medications...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32952584</th>\n",
       "      <td>Patient 10000032, a 52 year old white female, ...</td>\n",
       "      <td>The ED disposition was home at 2180-07-23 05:5...</td>\n",
       "      <td>The patient was admitted at 2180-07-23 12:35:00.</td>\n",
       "      <td>The patient's discharge disposition was: home ...</td>\n",
       "      <td>h o m e</td>\n",
       "      <td>At triage: temperature was 97.8, pulse was 87....</td>\n",
       "      <td>The patient was previously taking the followin...</td>\n",
       "      <td>The patient had the following vitals: At 2180-...</td>\n",
       "      <td>The patient received the following diagnostic ...</td>\n",
       "      <td>The patient received the following medications...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39399961</th>\n",
       "      <td>Patient 10000032, a 52 year old white female, ...</td>\n",
       "      <td>The ED disposition was admitted at 2180-07-23 ...</td>\n",
       "      <td>The patient was admitted at 2180-07-23 12:35:00.</td>\n",
       "      <td>The patient's discharge disposition was: home ...</td>\n",
       "      <td>a d m i t t e d</td>\n",
       "      <td>At triage: temperature was 98.7, pulse was 77....</td>\n",
       "      <td>The patient was previously taking the followin...</td>\n",
       "      <td>The patient had the following vitals: At 2180-...</td>\n",
       "      <td>The patient received the following diagnostic ...</td>\n",
       "      <td>The patient received the following medications...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    arrival  \\\n",
       "33258284  Patient 10000032, a 52 year old white female, ...   \n",
       "38112554  Patient 10000032, a 52 year old white female, ...   \n",
       "35968195  Patient 10000032, a 52 year old white female, ...   \n",
       "32952584  Patient 10000032, a 52 year old white female, ...   \n",
       "39399961  Patient 10000032, a 52 year old white female, ...   \n",
       "\n",
       "                                                eddischarge  \\\n",
       "33258284  The ED disposition was admitted at 2180-05-06 ...   \n",
       "38112554  The ED disposition was admitted at 2180-06-26 ...   \n",
       "35968195  The ED disposition was admitted at 2180-08-06 ...   \n",
       "32952584  The ED disposition was home at 2180-07-23 05:5...   \n",
       "39399961  The ED disposition was admitted at 2180-07-23 ...   \n",
       "\n",
       "                                                 admission  \\\n",
       "33258284  The patient was admitted at 2180-05-06 22:23:00.   \n",
       "38112554  The patient was admitted at 2180-06-26 18:27:00.   \n",
       "35968195  The patient was admitted at 2180-08-05 23:44:00.   \n",
       "32952584  The patient was admitted at 2180-07-23 12:35:00.   \n",
       "39399961  The patient was admitted at 2180-07-23 12:35:00.   \n",
       "\n",
       "                                                  discharge  \\\n",
       "33258284  The patient's discharge disposition was: home ...   \n",
       "38112554  The patient's discharge disposition was: home ...   \n",
       "35968195  The patient's discharge disposition was: hospi...   \n",
       "32952584  The patient's discharge disposition was: home ...   \n",
       "39399961  The patient's discharge disposition was: home ...   \n",
       "\n",
       "         eddischarge_category  \\\n",
       "33258284      a d m i t t e d   \n",
       "38112554      a d m i t t e d   \n",
       "35968195      a d m i t t e d   \n",
       "32952584              h o m e   \n",
       "39399961      a d m i t t e d   \n",
       "\n",
       "                                                     triage  \\\n",
       "33258284  At triage: temperature was 98.4, pulse was 70....   \n",
       "38112554  At triage: temperature was 98.9, pulse was 88....   \n",
       "35968195  At triage: temperature was 99.4, pulse was 105...   \n",
       "32952584  At triage: temperature was 97.8, pulse was 87....   \n",
       "39399961  At triage: temperature was 98.7, pulse was 77....   \n",
       "\n",
       "                                                   medrecon  \\\n",
       "33258284  The patient was previously taking the followin...   \n",
       "38112554  The patient was previously taking the followin...   \n",
       "35968195  The patient was previously taking the followin...   \n",
       "32952584  The patient was previously taking the followin...   \n",
       "39399961  The patient was previously taking the followin...   \n",
       "\n",
       "                                                     vitals  \\\n",
       "33258284  The patient had the following vitals: At 2180-...   \n",
       "38112554  The patient had the following vitals: At 2180-...   \n",
       "35968195  The patient had the following vitals: At 2180-...   \n",
       "32952584  The patient had the following vitals: At 2180-...   \n",
       "39399961  The patient had the following vitals: At 2180-...   \n",
       "\n",
       "                                                      codes  \\\n",
       "33258284  The patient received the following diagnostic ...   \n",
       "38112554  The patient received the following diagnostic ...   \n",
       "35968195  The patient received the following diagnostic ...   \n",
       "32952584  The patient received the following diagnostic ...   \n",
       "39399961  The patient received the following diagnostic ...   \n",
       "\n",
       "                                                      pyxis  \n",
       "33258284                                                NaN  \n",
       "38112554  The patient received the following medications...  \n",
       "35968195  The patient received the following medications...  \n",
       "32952584  The patient received the following medications...  \n",
       "39399961  The patient received the following medications...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'chianglab-dataderivatives'\n",
    "file_path = \"mimic-iv-ed-2.2/text_repr.json\"\n",
    "\n",
    "# loading in raw data\n",
    "content_object = s3.Object(bucket_name, file_path)\n",
    "file_content = content_object.get()['Body'].read().decode('utf-8')\n",
    "json_content = json.loads(file_content)\n",
    "df = pd.DataFrame(json_content).T\n",
    "print(\"length of dataframe: \"+ str(len(df)))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32f9fa0-5ae5-4716-8f4d-a481cbd77b38",
   "metadata": {},
   "source": [
    "### fill in missing entries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e21cce55-4e76-4462-8131-56edf3a2f23a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['medrecon', 'vitals', 'codes', 'pyxis']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check which columns have nans\n",
    "df.columns[df.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b030d910-9c6a-4d6d-9a7c-8091d3f4e164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fix missing entries\n",
    "df['medrecon'] = df['medrecon'].fillna(\"The patient was previously not taking any medications.\")\n",
    "df['pyxis'] = df['pyxis'].fillna(\"The patient did not receive any medications.\")\n",
    "df['vitals'] = df['vitals'].fillna(\"The patient had no vitals recorded\")\n",
    "df['codes'] = df['codes'].fillna(\"The patient received no diagnostic codes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f367e9-17d8-4c46-a181-72f9007011b0",
   "metadata": {},
   "source": [
    "# Split Test & Training and write out Patients IDs for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3a1b1b2-91d5-41cc-8859-401f7a84f455",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70% Train: 280013 \n",
      "30% Test: 120006\n"
     ]
    }
   ],
   "source": [
    "# split the train and test split with seed so it is always the same\n",
    "def train_validate_test_split(df, train_percent=.7, validate_percent=.15, seed=7):\n",
    "    np.random.seed(seed) # set seed for reproducibility sake\n",
    "    df = df.reset_index()\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.iloc[perm[:train_end]]\n",
    "    validate = df.iloc[perm[train_end:validate_end]]\n",
    "    test = df.iloc[perm[validate_end:]]\n",
    "    train = train.set_index('index')\n",
    "    validate = validate.set_index('index')\n",
    "    test = test.set_index('index')\n",
    "    return train, validate, test\n",
    "\n",
    "train, validate, test = train_validate_test_split(df, seed=7)\n",
    "train2, validate2, test2 = train_validate_test_split(df, seed=7)\n",
    "print(\"70% Train:\",len(train), \"\\n30% Test:\",len(validate+test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fbae476-08ff-4d45-9bb2-bbac68ba1efb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lists are identical\n",
      "stored patient IDs into: ./models/data/train_patients.txt\n",
      "stored patient IDs into: ./models/data/test_patients.txt\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "# extracts the patient ID's from the arrival column \n",
    "train_patients = train.arrival.astype(str).str.split().str[1].to_list()\n",
    "train_patients2 = train2.arrival.astype(str).str.split().str[1].to_list()\n",
    "test_patients = test.arrival.astype(str).str.split().str[1].to_list()\n",
    "validate_patients = validate.arrival.astype(str).str.split().str[1].to_list()\n",
    "test_patients = (test_patients+validate_patients)\n",
    "\n",
    "# Sanity Check: checking if seed works by seeing if training sets are equal when called two separate times for future reproducibility\n",
    "train_patients.sort()\n",
    "train_patients2.sort()\n",
    "\n",
    "train_patients = [''.join(char for char in item if char not in string.punctuation) for item in train_patients]\n",
    "train_patients2 = [''.join(char for char in item if char not in string.punctuation) for item in train_patients2]\n",
    "test_patients = [''.join(char for char in item if char not in string.punctuation) for item in test_patients]\n",
    " \n",
    "# using == to check if lists are equal\n",
    "if train_patients == train_patients2:\n",
    "    print(\"The lists are identical\")\n",
    "else:\n",
    "    print(\"The lists are not identical\")\n",
    "\n",
    "# free up memory by deleting\n",
    "del train2\n",
    "del validate2\n",
    "del test2\n",
    "\n",
    "# remove duplicates    \n",
    "train_patients = set(train_patients)\n",
    "test_patients = set(test_patients)\n",
    "\n",
    "# write patient ID's into txt files for lookup purposes later in case there are dependency issues in the future that modify seeding\n",
    "file = open('./models/data/train_patients.txt','w')\n",
    "for patient in train_patients:\n",
    "\tfile.write(patient+\"\\n\")\n",
    "file.close()\n",
    "print(\"stored patient IDs into: ./models/data/train_patients.txt\")\n",
    "\n",
    "file = open('./models/data/test_patients.txt','w')\n",
    "for patient in test_patients:\n",
    "\tfile.write(patient+\"\\n\")\n",
    "file.close()\n",
    "print(\"stored patient IDs into: ./models/data/test_patients.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc4d9a5-9434-4931-b0eb-0d01b583498a",
   "metadata": {},
   "source": [
    "# Tokenize our Corpus\n",
    "\n",
    "We will use the biobert tokenizer to ensure that it covers some biomedical terminology found in both MIMIC and PubMed databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43674fac-6638-4685-bf96-d02ce2ba0c82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Stacked\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">36706089</th>\n",
       "      <th>arrival</th>\n",
       "      <td>Patient 17100154, a 19 year old white female, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eddischarge</th>\n",
       "      <td>The ED disposition was admitted at 2130-11-06 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>admission</th>\n",
       "      <td>The patient was admitted at 2130-11-06 04:56:00.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discharge</th>\n",
       "      <td>The patient's discharge disposition was: unkno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>triage</th>\n",
       "      <td>At triage: temperature was 97.6, pulse was 93....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medrecon</th>\n",
       "      <td>The patient was previously taking the followin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vitals</th>\n",
       "      <td>The patient had the following vitals: At 2130-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codes</th>\n",
       "      <td>The patient received the following diagnostic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyxis</th>\n",
       "      <td>The patient received the following medications...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38867343</th>\n",
       "      <th>arrival</th>\n",
       "      <td>Patient 17533742, a 42 year old black/african ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               headline\n",
       "index                                                                  \n",
       "36706089 arrival      Patient 17100154, a 19 year old white female, ...\n",
       "         eddischarge  The ED disposition was admitted at 2130-11-06 ...\n",
       "         admission     The patient was admitted at 2130-11-06 04:56:00.\n",
       "         discharge    The patient's discharge disposition was: unkno...\n",
       "         triage       At triage: temperature was 97.6, pulse was 93....\n",
       "         medrecon     The patient was previously taking the followin...\n",
       "         vitals       The patient had the following vitals: At 2130-...\n",
       "         codes        The patient received the following diagnostic ...\n",
       "         pyxis        The patient received the following medications...\n",
       "38867343 arrival      Patient 17533742, a 42 year old black/african ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Stacked\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">39515874</th>\n",
       "      <th>arrival</th>\n",
       "      <td>Patient 10264068, a 55 year old hispanic or la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eddischarge</th>\n",
       "      <td>The ED disposition was home at 2124-10-31 14:1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>admission</th>\n",
       "      <td>The patient was admitted at 2124-10-31 03:40:00.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discharge</th>\n",
       "      <td>The patient's discharge disposition was: unkno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>triage</th>\n",
       "      <td>At triage: temperature was 98.5, pulse was 71....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medrecon</th>\n",
       "      <td>The patient was previously not taking any medi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vitals</th>\n",
       "      <td>The patient had no vitals recorded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codes</th>\n",
       "      <td>The patient received the following diagnostic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyxis</th>\n",
       "      <td>The patient received the following medications...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34432494</th>\n",
       "      <th>arrival</th>\n",
       "      <td>Patient 18457617, a 31 year old white female, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               headline\n",
       "index                                                                  \n",
       "39515874 arrival      Patient 10264068, a 55 year old hispanic or la...\n",
       "         eddischarge  The ED disposition was home at 2124-10-31 14:1...\n",
       "         admission     The patient was admitted at 2124-10-31 03:40:00.\n",
       "         discharge    The patient's discharge disposition was: unkno...\n",
       "         triage       At triage: temperature was 98.5, pulse was 71....\n",
       "         medrecon     The patient was previously not taking any medi...\n",
       "         vitals                      The patient had no vitals recorded\n",
       "         codes        The patient received the following diagnostic ...\n",
       "         pyxis        The patient received the following medications...\n",
       "34432494 arrival      Patient 18457617, a 31 year old white female, ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feed it into a custom tokenizer but first need to make a Dataset Object for transformers\n",
    "disposition_train = train.eddischarge_category\n",
    "train = train.drop(\"eddischarge_category\",axis=1)\n",
    "stacked_train = train.stack().to_frame(\"headline\")\n",
    "print(\"Train Stacked\")\n",
    "display(stacked_train.head(10))\n",
    "\n",
    "disposition_test = test.eddischarge_category\n",
    "test = test.drop(\"eddischarge_category\",axis=1)\n",
    "stacked_test = test.stack().to_frame(\"headline\")\n",
    "print(\"Test Stacked\")\n",
    "display(stacked_test.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ecd30b3-f665-44c4-ac6d-fbd7480adc6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['headline', 'index', '__index_level_1__'],\n",
       "    num_rows: 2520117\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_corpus = Dataset.from_pandas(stacked_train)\n",
    "testing_data_corpus = Dataset.from_pandas(stacked_test)\n",
    "training_data_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76fdc0e-1179-4314-8862-97334c538ebb",
   "metadata": {},
   "source": [
    "### If we want to use an exisiting tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f1f7c47-a569-4681-8cc3-fcdb6bf080e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load tokenizer trained from biobert + pubmed200kRCT dataset\n",
    "run = False\n",
    "\n",
    "if run:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"pritamdeka/BioBert-PubMed200kRCT\")\n",
    "    def encode_with_truncation(examples):\n",
    "      \"\"\"Mapping function to tokenize the sentences passed with truncation\"\"\"\n",
    "      return tokenizer(examples[\"headline\"], truncation=True, padding=\"max_length\",\n",
    "                        max_length=512, return_special_tokens_mask=True)\n",
    "\n",
    "    def encode_without_truncation(examples):\n",
    "      \"\"\"Mapping function to tokenize the sentences passed without truncation\"\"\"\n",
    "      return tokenizer(examples[\"headline\"], return_special_tokens_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "674edc65-cefa-4ca9-a753-3648bbe88221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizing input\n",
    "#\n",
    "# Implementation: I tokenized without truncation so then we aren't cutting off our input from parts of the EHR for training\n",
    "if run:\n",
    "    truncate = False\n",
    "    if truncate:\n",
    "        print(\"Tokenizing with truncation\")\n",
    "        train_data_tokenized = training_data_corpus.map(encode_without_truncation, batched=True)\n",
    "        train_data_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "        print(\"Tokenizing Complete\")\n",
    "    else:\n",
    "        print(\"Tokenizing without truncation\")\n",
    "        train_data_tokenized = training_data_corpus.map(encode_without_truncation, batched=True)\n",
    "        train_data_tokenized.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "        print(\"Tokenizing Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0037cf8-08c4-482a-9724-cf2eee7263d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "if run:\n",
    "    # Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "    # max_seq_length.\n",
    "    # grabbed from: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
    "    def group_texts(examples):\n",
    "        # Concatenate all texts.\n",
    "        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "        if total_length >= max_length:\n",
    "            total_length = (total_length // max_length) * max_length\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n",
    "    # remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n",
    "    # might be slower to preprocess.\n",
    "    #\n",
    "    # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "\n",
    "    max_length = 512\n",
    "    if not truncate:\n",
    "        train_dataset = train_data_tokenized.map(group_texts, batched=True, desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "        # convert them from lists to torch tensors\n",
    "        train_dataset.set_format(\"torch\")\n",
    "\n",
    "    print(\"data preprocessing is finished\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542880c6-16eb-403b-928b-26310b5c9a82",
   "metadata": {},
   "source": [
    "# Creating our own Tokenizer for RoBERTa Model\n",
    "\n",
    "Why Pretrain a RoBERTa over a traditional BERT:\n",
    "\n",
    "*\"This way, in BERT, the masking is performed only once at data preparation time, and they basically take each sentence and mask it in 10 different ways. Therefore, at training time, the model will only see those 10 variations of each sentence.*\n",
    "\n",
    "*On the other hand, in RoBERTa, the masking is done during training. Therefore, each time a sentence is incorporated in a minibatch, it gets its masking done, and therefore the number of potentially different masked versions of each sentence is not bounded like in BERT.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1159745-61e9-432c-a64a-a180d14d3eab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt_files_dir = \"./models/data/text_split/\"\n",
    "!rm -rf {txt_files_dir}\n",
    "!mkdir {txt_files_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ff501ea-7d23-4ca9-a964-bc261a437ce0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "# if you want to train the tokenizer from scratch (especially if you have custom\n",
    "# dataset loaded as datasets object), then run this cell to save it as files\n",
    "# but if you already have your custom data as text files, there is no point using this\n",
    "def column_to_files(column, txt_files_dir,output_filename=\"train.txt\"):\n",
    "    # The prefix is a unique ID to avoid to overwrite a text file\n",
    "    i=1\n",
    "    counter = 0\n",
    "    #For every value in the df, with just one column\n",
    "    for row in tqdm(column.to_list()):\n",
    "      # Create the filename using the prefix ID\n",
    "        if i % 1000 == 1:\n",
    "            file_name = os.path.join(txt_files_dir, str(counter)+'.txt')\n",
    "            f = open(file_name, 'wb')\n",
    "        try:\n",
    "            f.write(row.encode('utf-8'))\n",
    "            if i % 1000 == 0:\n",
    "                f.close()\n",
    "                counter += 1\n",
    "        except Exception as e:  #catch exceptions(for eg. empty rows)\n",
    "            print(row, e) \n",
    "        i+=1\n",
    "    # Return the last ID\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98a0db49-e264-4eaf-b894-2b7b8d4f7dbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2520117/2520117 [01:11<00:00, 35050.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turned train dataset into txt file\n"
     ]
    }
   ],
   "source": [
    "# Get the training data\n",
    "training_data = stacked_train[\"headline\"]\n",
    "# Removing the end of line character \\n\n",
    "training_data = training_data.replace(\"\\n\",\" \")\n",
    "# Create a file for every description value\n",
    "train_num_files = column_to_files(training_data, txt_files_dir, output_filename=\"train.txt\")\n",
    "print(\"Turned train dataset into txt file\")\n",
    "\n",
    "# # Get the test data\n",
    "# test_data = stacked_test[\"headline\"]\n",
    "# # Removing the end of line character \\n\n",
    "# test_data = test_data.replace(\"\\n\",\" \")\n",
    "# # Create a file for every description value\n",
    "# test_num_files = column_to_files(test_data, txt_files_dir, output_filename=\"test.txt\")\n",
    "# print(\"Turned test dataset into txt file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba9f92a5-c788-4840-bf4e-fc4d14ee37b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer_dir = \"./models/data/TokenizerRoBERTa\"\n",
    "!rm -rf {tokenizer_dir}\n",
    "!mkdir {tokenizer_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9befea8-c257-4ff1-b20d-8e165e73b66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Dataset: 2521\n",
      "Train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./models/data/TokenizerRoBERTa/vocab.json',\n",
       " './models/data/TokenizerRoBERTa/merges.txt']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from pathlib import Path\n",
    "\n",
    "paths = [str(x) for x in Path(\".\").glob(\"./models/data/text_split/*.txt\")]\n",
    "print(\"Loaded Dataset:\", str(len(paths)))\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
    "\n",
    "print(\"Train\")\n",
    "# Customize training\n",
    "\n",
    "# we choose a vocab_size of 30,522 to reduce the OOV tokens which may commonly be found in Medical Terminology\n",
    "tokenizer.train(files=paths, vocab_size=30_522, min_frequency=2,\n",
    "                show_progress=True,\n",
    "                special_tokens=[\n",
    "                                \"<s>\",\n",
    "                                \"<pad>\",\n",
    "                                \"</s>\",\n",
    "                                \"<unk>\",\n",
    "                                \"<mask>\",\n",
    "])\n",
    "#Save the Tokenizer to disk\n",
    "tokenizer.save_model(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77e9bd53-74ac-4283-b4ce-95f588cf894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the batched dataset because we don't want to accidently upload all that data\n",
    "!rm -rf {txt_files_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a55c27-cff2-4f44-8dad-2c90cd27055e",
   "metadata": {},
   "source": [
    "Making sure Tokenizer works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57a77e5f-f688-4c62-baa8-4d9f2c841d6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'T',\n",
       " 'he',\n",
       " 'patient',\n",
       " 'was',\n",
       " 'previously',\n",
       " 'not',\n",
       " 'taking',\n",
       " 'any',\n",
       " 'medications',\n",
       " '.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the tokenizer using vocab.json and mrege.txt files\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    os.path.abspath(os.path.join(tokenizer_dir,'vocab.json')),\n",
    "    os.path.abspath(os.path.join(tokenizer_dir,'merges.txt'))\n",
    ")\n",
    "# Prepare the tokenizer\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "# Test the tokenizer\n",
    "tokenizer.encode(\"The patient was previously not taking any medications.\")\n",
    "# Show the tokens created\n",
    "tokenizer.encode(\"The patient was previously not taking any medications.\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07c608a-2bbb-427c-9fe5-cf795cb0bc05",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5037b9b4-b529-48fe-a92a-a4ca52cc2d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters:  66987834\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "# Set a configuration for our RoBERTa model\n",
    "config = RobertaConfig(\n",
    "    vocab_size=30522,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "# Initialize the model from a configuration without pretrained weights\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "print('Num parameters: ',model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5f96c42-5f74-44ba-81ba-065ad72f540b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "# Create the tokenizer from our trained one\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_dir, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11472ced-bc70-4056-85a4-c0d6dee6a4ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaTokenizerFast(name_or_path='./models/data/TokenizerRoBERTa', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c552b87-634a-4bbf-ae10-d5a7a397418e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_with_truncation(examples):\n",
    "  \"\"\"Mapping function to tokenize the sentences passed with truncation\"\"\"\n",
    "  return tokenizer(examples[\"headline\"], truncation=True, padding=\"max_length\",\n",
    "                    max_length=512, return_special_tokens_mask=True)\n",
    "\n",
    "def encode_without_truncation(examples):\n",
    "  \"\"\"Mapping function to tokenize the sentences passed without truncation\"\"\"\n",
    "  return tokenizer(examples[\"headline\"], return_special_tokens_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7919eb15-0c8d-4b1f-89eb-2d0a83164207",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing without truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2520117 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (778 > 512). Running this sequence through the model will result in indexing errors\n",
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Tokenizing Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "truncate = False\n",
    "if truncate:\n",
    "    print(\"Tokenizing with truncation\")\n",
    "    train_data_tokenized = training_data_corpus.map(encode_without_truncation, batched=True)\n",
    "    train_data_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    print(\"Tokenizing Complete\")\n",
    "else:\n",
    "    print(\"Tokenizing without truncation\")\n",
    "    train_data_tokenized = training_data_corpus.map(encode_without_truncation, batched=True)\n",
    "    train_data_tokenized.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "    print(\"Training Tokenizing Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7c6f19c-9e9e-44da-b85f-93ee806d9c40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing without truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Tokenizing Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "truncate = False\n",
    "if truncate:\n",
    "    print(\"Tokenizing with truncation\")\n",
    "    train_data_tokenized = training_data_corpus.map(encode_without_truncation, batched=True)\n",
    "    train_data_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    print(\"Tokenizing Complete\")\n",
    "else:\n",
    "    print(\"Tokenizing without truncation\")\n",
    "    test_data_tokenized = testing_data_corpus.map(encode_without_truncation, batched=True)\n",
    "    test_data_tokenized.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "    print(\"Testing Tokenizing Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a00150bd-bcd4-4f40-80b7-e0394bac4fed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data preprocessing is finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# max_seq_length.\n",
    "# grabbed from: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= max_length:\n",
    "        total_length = (total_length // max_length) * max_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n",
    "# remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n",
    "# might be slower to preprocess.\n",
    "#\n",
    "# To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "# https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "\n",
    "max_length = 512\n",
    "if not truncate:\n",
    "    train_dataset = train_data_tokenized.map(group_texts, batched=True, desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "    # convert them from lists to torch tensors\n",
    "    train_dataset.set_format(\"torch\")\n",
    "    test_dataset = test_data_tokenized.map(group_texts, batched=True, desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "    # convert them from lists to torch tensors\n",
    "    test_dataset.set_format(\"torch\")\n",
    "\n",
    "print(\"data preprocessing is finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5d2c025-7824-4df6-874e-aafc4fdc3bba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_dataset = test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58a78ab1-dbe7-4426-b805-93d8ecaf8d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        # or use the RobertaTokenizer from `transformers` directly.\n",
    "        self.examples = []\n",
    "        # For every value in the dataframe \n",
    "        for example in tqdm(df.values):\n",
    "            # \n",
    "            x=tokenizer.encode_plus(example,max_length = 512, truncation=True, padding=True)\n",
    "            self.examples += [x.input_ids]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Well pad at the batch level.\n",
    "        return torch.tensor(self.examples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37e57d8b-9fcb-4e87-a4eb-19bfe76d4478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the train and evaluation dataset\n",
    "if run:\n",
    "    train_dataset = CustomDataset(stacked_train['headline'], tokenizer)\n",
    "    eval_dataset = CustomDataset(stacked_test['headline'], tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262ca5b6-e103-48d0-8be3-18fc3c20c468",
   "metadata": {},
   "source": [
    "# Defining the data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "823160d7-0255-42f1-9574-f88b1fc64ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Define the Data Collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9300af-1de1-4c73-83be-a97550037377",
   "metadata": {},
   "source": [
    "# Intialize and train the Trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f586442-1109-4e19-9e0e-86a52a94feea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 32    # input batch size for training (default: 64)\n",
    "VALID_BATCH_SIZE = 32    # input batch size for testing (default: 1000)\n",
    "TRAIN_EPOCHS = 5        # number of epochs to train (default: 10)\n",
    "LEARNING_RATE = 1e-4    # learning rate (default: 0.001)\n",
    "WEIGHT_DECAY = 0.01\n",
    "SEED = 42               # random seed (default: 42)\n",
    "MAX_LEN = 128\n",
    "SUMMARY_LEN = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc57db23-c975-49c8-ae84-d8e27379dec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dir = \"./models/EHR-RoBERTa\"\n",
    "!rm -rf {model_dir}\n",
    "!mkdir {model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "48c45771-7f86-4660-9278-6de84c801ca2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 1000\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/EHR-RoBERTa\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "print(model_dir)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = 'steps',\n",
    "    num_train_epochs=TRAIN_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=VALID_BATCH_SIZE,\n",
    "    logging_steps=5000,             # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=5000,\n",
    "    #eval_steps=4096,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "# Create the trainer for our model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    #prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36d9864-376b-45b8-9b00-73421615a407",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d38ee7bd-d32c-4727-ab7d-70e47a702a17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask, __index_level_1__, index, headline. If special_tokens_mask, __index_level_1__, index, headline are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1201829\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 375580\n",
      "  Number of trainable parameters = 66987834\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='375580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [     6/375580 00:55 < 1447:33:29, 0.07 it/s, Epoch 0.00/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">&lt;ipython-input-47-3435b262f1ae&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1543</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1540       </span>inner_training_loop = find_executable_batch_size(                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1541          </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._inner_training_loop, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_batch_size, args.auto_find_batch_size  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1542       </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1543 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> inner_training_loop(                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1544          </span>args=args,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1545          </span>resume_from_checkpoint=resume_from_checkpoint,                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1546          </span>trial=trial,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1791</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_training_loop</span>      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1788                </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> model.no_sync():                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1789                   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1790             </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1791 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1792             </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1793             </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> (                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1794                </span>args.logging_nan_inf_filter                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2539</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">training_step</span>             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2536          </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> loss_mb.reduce_mean().detach().to(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.args.device)                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2537       </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2538       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.compute_loss_context_manager():                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>2539 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>loss = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.compute_loss(model, inputs)                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2540       </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2541       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.args.n_gpu &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>:                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2542          </span>loss = loss.mean()  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># mean() to average on multi-gpu parallel training</span>        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2571</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">compute_loss</span>              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2568          </span>labels = inputs.pop(<span style=\"color: #808000; text-decoration-color: #808000\">\"labels\"</span>)                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2569       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2570          </span>labels = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>2571 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>outputs = model(**inputs)                                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2572       # Save past state if it exists</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2573       # TODO: this needs to be fixed and made cleaner later.</span>                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2574       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.args.past_index &gt;= <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>:                                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1190</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1187       # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1188       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1189             </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1190 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1191       # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192       </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/transformers/models/roberta/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_roberta.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1111</span> in   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1108          </span>return_dict=return_dict,                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1109       </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1110       </span>sequence_output = outputs[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1111 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>prediction_scores = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.lm_head(sequence_output)                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1112       </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1113       </span>masked_lm_loss = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1114       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> labels <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1190</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1187       # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1188       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1189             </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1190 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1191       # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192       </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/transformers/models/roberta/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_roberta.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1148</span> in   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1145       </span>x = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.layer_norm(x)                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1146       </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1147       # project back to size of vocabulary with bias</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1148 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>x = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.decoder(x)                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1149       </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1150       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> x                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1151 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1190</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1187       # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1188       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1189             </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1190 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1191       # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192       </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">linear.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">114</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">111          </span>init.uniform_(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bias, -bound, bound)                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112    </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113    </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>: Tensor) -&gt; Tensor:                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>114 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> F.linear(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.weight, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bias)                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">115    </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116    </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">extra_repr</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>) -&gt; <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>:                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #808000; text-decoration-color: #808000\">'in_features={}, out_features={}, bias={}'</span>.format(                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33m<ipython-input-47-3435b262f1ae>\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<module>\u001b[0m                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1543\u001b[0m in \u001b[92mtrain\u001b[0m                     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1540 \u001b[0m\u001b[2m      \u001b[0minner_training_loop = find_executable_batch_size(                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1541 \u001b[0m\u001b[2m         \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.auto_find_batch_size  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1542 \u001b[0m\u001b[2m      \u001b[0m)                                                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1543 \u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1544 \u001b[0m\u001b[2m         \u001b[0margs=args,                                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1545 \u001b[0m\u001b[2m         \u001b[0mresume_from_checkpoint=resume_from_checkpoint,                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1546 \u001b[0m\u001b[2m         \u001b[0mtrial=trial,                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1791\u001b[0m in \u001b[92m_inner_training_loop\u001b[0m      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1788 \u001b[0m\u001b[2m               \u001b[0m\u001b[94mwith\u001b[0m model.no_sync():                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1789 \u001b[0m\u001b[2m                  \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1790 \u001b[0m\u001b[2m            \u001b[0m\u001b[94melse\u001b[0m:                                                                     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1791 \u001b[2m               \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1792 \u001b[0m\u001b[2m            \u001b[0m                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1793 \u001b[0m\u001b[2m            \u001b[0m\u001b[94mif\u001b[0m (                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1794 \u001b[0m\u001b[2m               \u001b[0margs.logging_nan_inf_filter                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2539\u001b[0m in \u001b[92mtraining_step\u001b[0m             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m2536 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m loss_mb.reduce_mean().detach().to(\u001b[96mself\u001b[0m.args.device)                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m2537 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m2538 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mself\u001b[0m.compute_loss_context_manager():                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m2539 \u001b[2m         \u001b[0mloss = \u001b[96mself\u001b[0m.compute_loss(model, inputs)                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m2540 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m2541 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.args.n_gpu > \u001b[94m1\u001b[0m:                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m2542 \u001b[0m\u001b[2m         \u001b[0mloss = loss.mean()  \u001b[2m# mean() to average on multi-gpu parallel training\u001b[0m        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2571\u001b[0m in \u001b[92mcompute_loss\u001b[0m              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m2568 \u001b[0m\u001b[2m         \u001b[0mlabels = inputs.pop(\u001b[33m\"\u001b[0m\u001b[33mlabels\u001b[0m\u001b[33m\"\u001b[0m)                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m2569 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m2570 \u001b[0m\u001b[2m         \u001b[0mlabels = \u001b[94mNone\u001b[0m                                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m2571 \u001b[2m      \u001b[0moutputs = model(**inputs)                                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m2572 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# Save past state if it exists\u001b[0m                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m2573 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# TODO: this needs to be fixed and made cleaner later.\u001b[0m                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m2574 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.args.past_index >= \u001b[94m0\u001b[0m:                                                     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1190\u001b[0m in \u001b[92m_call_impl\u001b[0m             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1187 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1188 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1189 \u001b[0m\u001b[2m            \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1190 \u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1191 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m      \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/transformers/models/roberta/\u001b[0m\u001b[1;33mmodeling_roberta.py\u001b[0m:\u001b[94m1111\u001b[0m in   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[92mforward\u001b[0m                                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1108 \u001b[0m\u001b[2m         \u001b[0mreturn_dict=return_dict,                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1109 \u001b[0m\u001b[2m      \u001b[0m)                                                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1110 \u001b[0m\u001b[2m      \u001b[0msequence_output = outputs[\u001b[94m0\u001b[0m]                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1111 \u001b[2m      \u001b[0mprediction_scores = \u001b[96mself\u001b[0m.lm_head(sequence_output)                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1112 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1113 \u001b[0m\u001b[2m      \u001b[0mmasked_lm_loss = \u001b[94mNone\u001b[0m                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1114 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m labels \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1190\u001b[0m in \u001b[92m_call_impl\u001b[0m             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1187 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1188 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1189 \u001b[0m\u001b[2m            \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1190 \u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1191 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m      \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/transformers/models/roberta/\u001b[0m\u001b[1;33mmodeling_roberta.py\u001b[0m:\u001b[94m1148\u001b[0m in   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[92mforward\u001b[0m                                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1145 \u001b[0m\u001b[2m      \u001b[0mx = \u001b[96mself\u001b[0m.layer_norm(x)                                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1146 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1147 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# project back to size of vocabulary with bias\u001b[0m                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1148 \u001b[2m      \u001b[0mx = \u001b[96mself\u001b[0m.decoder(x)                                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1149 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1150 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m x                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1151 \u001b[0m                                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1190\u001b[0m in \u001b[92m_call_impl\u001b[0m             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1187 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1188 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1189 \u001b[0m\u001b[2m            \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1190 \u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1191 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m      \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mlinear.py\u001b[0m:\u001b[94m114\u001b[0m in \u001b[92mforward\u001b[0m                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m         \u001b[0minit.uniform_(\u001b[96mself\u001b[0m.bias, -bound, bound)                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m   \u001b[0m                                                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, \u001b[96minput\u001b[0m: Tensor) -> Tensor:                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m114 \u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m F.linear(\u001b[96minput\u001b[0m, \u001b[96mself\u001b[0m.weight, \u001b[96mself\u001b[0m.bias)                                     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m115 \u001b[0m\u001b[2m   \u001b[0m                                                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mextra_repr\u001b[0m(\u001b[96mself\u001b[0m) -> \u001b[96mstr\u001b[0m:                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m \u001b[33m'\u001b[0m\u001b[33min_features=\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m, out_features=\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m, bias=\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m'\u001b[0m.format(                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae73395f-5a4a-4c39-9dac-e17958bb9ae6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model(model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c91f6e-a0e3-4925-b292-47ceb1b03ebd",
   "metadata": {},
   "source": [
    "#### Perplexity, AUROC, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e4f4a3-4eeb-43bf-9e67-ccc92bc214cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec650e87-96d6-448c-9981-a85846a3287d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.13 Python 3.9 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/pytorch-1.13-gpu-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
